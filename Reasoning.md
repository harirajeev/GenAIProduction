- Specialized reasoning models such as DeepSeek-R1, OpenAI's o1 & o3
  - The exact workings of o1 and o3 remain unknown outside of OpenAI. However, they are rumored to leverage a combination of both inference and training techniques.
  - three key approaches to building and improving reasoning models:
  - 1) Inference-time scaling
       - refers to increasing computational resources during inference to improve output quality.
       - rough analogy is how humans tend to generate better responses when given more time to think through complex problems.
       - chain-of-thought (CoT) prompting
       - use of voting and search strategies
       - Inference-time scaling, a technique that improves reasoning capabilities without training or otherwise modifying the underlying model.
       - I suspect that OpenAI's o1 and o3 models use inference-time scaling, which would explain why they are relatively expensive compared to models like GPT-4o. In addition to inference-time scaling, o1 and o3 were likely trained using RL pipelines similar to those used for DeepSeek R1. More on reinforcement learning in the next two sections below.
  - 2) Pure reinforcement learning (RL)
       - reasoning emerges as a behavior from pure reinforcement learning (RL) - discovery of DeepSeek-R1
       - DeepSeek-R1-Zero was trained exclusively with reinforcement learning without an initial SFT stage
       - it is possible to develop a reasoning model using pure RL, and the DeepSeek team was the first to demonstrate
       - ure reinforcement learning (RL) as in DeepSeek-R1-Zero, which showed that reasoning can emerge as a learned behavior without supervised fine-tuning.
  - 3) Supervised fine-tuning (SFT) plus RL, which led to DeepSeek-R1, DeepSeek’s flagship reasoning model.
  - 4) Model "distillation."
       - DeepSeek also released smaller models trained via a process they call distillation.
       - in the context of LLMs, distillation does not necessarily follow the classical knowledge distillation approach used in deep learning.
         - a smaller student model is trained on both the logits of a larger teacher model and a target dataset.
       - here distillation refers to instruction fine-tuning smaller LLMs, such as Llama 8B and 70B and Qwen 2.5 models (0.5B to 32B), on an SFT dataset generated by larger LLMs.
       - in fact, the SFT data used for this distillation process is the same dataset that was used to train DeepSeek-R1
       - Smaller models are more efficient. This means they are cheaper to run, but they also can run on lower-end hardware

DeepSeek
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning - Paper](https://arxiv.org/abs/2501.12948)
-  [The Illustrated DeepSeek-R1 - Jay Allamar](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

 ![image](https://github.com/user-attachments/assets/48fcb97a-e9d3-4def-bc58-737868c567b7)


- [Understanding Reasoning LLMs - Sebastian Raschka](https://www.linkedin.com/pulse/understanding-reasoning-llms-sebastian-raschka-phd-1tshc/?trackingId=IUjZ42GWSOmZmtdNX5lBRw%3D%3D)
         
  - DeepSeek did not release a single R1 reasoning model but instead introduced three distinct variants: DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill.
   ![image](https://github.com/user-attachments/assets/3a917d7a-97ba-4951-be0e-b42b53ee29fe)
  - (1) DeepSeek-R1-Zero: This model is based on the 671B pre-trained DeepSeek-V3 base model released in December 2024. The research team trained it using reinforcement learning (RL) with two types of rewards. This approach is referred to as "cold start" training because it did not include a supervised fine-tuning (SFT) step, which is typically part of reinforcement learning with human feedback (RLHF). DeepSeek team used DeepSeek-R1-Zero to generate what they call "cold-start" SFT data. The term "cold start" refers to the fact that this data was produced by DeepSeek-R1-Zero, which itself had not been trained on any supervised fine-tuning (SFT) data.
  - (2) DeepSeek-R1: This is DeepSeek's flagship reasoning model, built upon DeepSeek-R1-Zero. The team further refined it with additional SFT stages and further RL training, improving upon the "cold-started" R1-Zero model.  Using this cold-start SFT data, DeepSeek then trained the model via instruction fine-tuning, followed by another reinforcement learning (RL) stage. The most recent model checkpoint was used to generate 600K Chain-of-Thought (CoT) SFT examples, while an additional 200K knowledge-based SFT examples were created using the DeepSeek-V3 base model. These 600K + 200K SFT samples were then used for another round of RL. 
  - (3) DeepSeek-R1-Distill*: Using the SFT data generated in the previous steps, the DeepSeek team fine-tuned Qwen and Llama models to enhance their reasoning abilities. While not distillation in the traditional sense, this process involved training smaller models (Llama 8B and 70B, and Qwen 1.5B–30B) on outputs from the larger DeepSeek-R1 671B model. 
    
Four different strategies for building and improving reasoning models:

1. Inference-time scaling requires no additional training but increases inference costs, making large-scale deployment more expensive as the number or users or query volume grows. Still, it remains a no-brainer for improving the performance of already strong models. I strongly suspect that o1 leverages inference-time scaling, which helps explain why it is more expensive on a per-token basis compared to DeepSeek-R1.

2. Pure RL is interesting for research purposes because it provides insights into reasoning as an emergent behavior. However, in practical model development, RL + SFT is the preferred approach as it leads to stronger reasoning models. I strongly suspect that o1 was trained using RL + SFT as well. More precisely, I believe o1 starts from a weaker, smaller base model than DeepSeek-R1 but compensates with RL + SFT and inference-time scaling.

3. As mentioned above, RL + SFT is the key approach for building high-performance reasoning models. DeepSeek-R1 is a nice blueprint showing how this can be done.

4. Distillation is an attractive approach, especially for creating smaller, more efficient models. However, the limitation is that distillation does not drive innovation or produce the next generation of reasoning models. For instance, distillation always depends on an existing, stronger model to generate the supervised fine-tuning (SFT) data.

One interesting aspect I expect to see next is to combine RL + SFT (approach 3) with inference-time scaling (approach 1). This is likely what OpenAI o1 is doing, except it's probably based on a weaker base model than DeepSeek-R1, which explains why DeepSeek-R1 performs so well while remaining relatively cheap at inference time.
