AI Red Teaming is a systematic, adversarial approach, employed by human testers, to identify issues/problems in systems that have Generative AI components viz.unsafe material, Inaccuracies, Out-of-scope responses & identify risks unknown at the time of development testing, that come to light from live usage/discovery of new vulnerabilities/new benchmarks. 

[OWASP Top 10 for LLMs - AI Red Teaming & Evaluation Guidelines v3](https://docs.google.com/document/d/1m06DMhonGuq8hTN30S-fAsuBA-ZK1UHMyzZamsZSTaE/edit)
